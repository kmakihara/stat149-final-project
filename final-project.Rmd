---
title: 'STAT 149: Final Project'
author: "Kazuma Makihara and Bobby Byung-Hoon Min"
date: "4/17/2018"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### na.convert.mean from lecture notes
```{r include=FALSE}
#### Include R Scripts without including in the markdown ####
na.convert.mean = function (frame) 
{
    vars <- names(frame)
    if (!is.null(resp <- attr(attr(frame, "terms"), "response"))) {
        vars <- vars[-resp]
        x <- frame[[resp]]
        pos <- is.na(x)
        if (any(pos)) {
            frame <- frame[!pos, , drop = FALSE]
            warning(paste(sum(pos), "observations omitted due to missing values in the response"))
        }
    }
    for (j in vars) {  #j is variable names
        x <- frame[[j]]
        pos <- is.na(x)
        if (any(pos)) {
            if (length(levels(x))) {   # factors
                xx <- as.character(x)
                xx[pos] <- "NA"
                x <- factor(xx, exclude = NULL)
            }
            else if (is.matrix(x)) {   # matrices
                ats <- attributes(x)
                x.na <- 1*pos
#               x[pos] <- 0
                w <- !pos
                n <- nrow(x)
                TT <- array(1, c(1, n))
                xbar <- (TT %*% x)/(TT %*% w)
                xbar <- t(TT) %*% xbar
                x[pos] <- xbar[pos]
                attributes(x) <- ats
                attributes(x.na) <- ats
                dimnames(x.na)[[2]]=paste(dimnames(x)[[2]],".na",sep='')
                frame[[paste(j,".na",sep='')]] <- x.na 
            } else {   # ordinary numerical vector
                ats <- attributes(x)
                x[pos] <- mean(x[!pos])
#               x[pos] <- 0
                x.na <- 1*pos
                frame[[paste(j,".na",sep='')]] <- x.na 
                attributes(x) <- ats
            }
            frame[[j]] <- x
        }
    }
    frame
}
```

#### stepwise_deviance_selector takes in a mytrain and a list of already selected_predictors 
```{r}
stepwise_deviance_selector = function (mytrain, selected_predictors, log=FALSE, alpha = 0.05) 
{
  model0 = glm(formula = paste("suppdem ~ ", do.call(paste, c(as.list(selected_predictors), sep = " + "))),
            family = binomial, data = mytrain)
  pval = alpha
  varname = ""
  missing = FALSE
  na.indicator = FALSE
  if (log == TRUE){
    mytrain$census_median_income = log(mytrain$census_median_income)
  }
  for (var in names(mytrain)){
    if (var == "suppdem" | var %in% selected_predictors) next
    if(anyNA(mytrain[[var]])) {
      na.var = paste(var, ".na", sep="")
      selected = c(var, selected_predictors)
      selected.na = c(var, na.var, selected_predictors)
      if (log == TRUE){
        mytrain.log.na <- mytrain.na
        mytrain.log.na$census_median_income = log(mytrain.log.na$census_median_income)
        mytrain.log.na$ppi = log(mytrain.log.na$ppi)
        model = glm(formula = paste("suppdem ~ ", do.call(paste, c(as.list(selected), sep = " + "))),
                    family = binomial, data = mytrain.log.na)
        modelna.na = glm(formula = paste("suppdem ~ ", do.call(paste, c(as.list(selected.na), sep = " + "))),
                    family = binomial, data = mytrain.log.na)
        if (anova(model, modelna.na, test="Chisq")$`Pr(>Chi)`[2] < 0.05) {
          model = modelna.na
          na.indicator = TRUE
        }
        if (anova(model0, model, test="Chisq")$`Pr(>Chi)`[2] < pval){
          varname = var
          pval = anova(model0, model, test="Chisq")$`Pr(>Chi)`[2]
          missing = TRUE
        }
      }
      else{
        mytrain.na = na.convert.mean(mytrain)
        model = glm(formula = paste("suppdem ~ ", do.call(paste, c(as.list(selected), sep = " + "))),
                    family = binomial, data = mytrain.na)
        modelna.na = glm(formula = paste("suppdem ~ ", do.call(paste, c(as.list(selected.na), sep = " + "))),
                    family = binomial, data = mytrain.na)
        if (anova(model, modelna.na, test="Chisq")$`Pr(>Chi)`[2] < 0.05) {
          model = modelna.na
          na.indicator = TRUE
        }
        if (anova(model0, model, test="Chisq")$`Pr(>Chi)`[2] < pval){
          varname = var
          pval = anova(model0, model, test="Chisq")$`Pr(>Chi)`[2]
          missing = TRUE
        }
      }
      
    }
    else{
      selected = c(var, selected_predictors)
      model = glm(formula = paste("suppdem ~ ", do.call(paste, c(as.list(selected), sep = " + ")))
                  , family = binomial, data = mytrain)
      if (anova(model0, model, test="Chisq")$`Pr(>Chi)`[2] < pval){
        varname = var
        pval = anova(model0, model, test="Chisq")$`Pr(>Chi)`[2]
        missing = FALSE
      }
    }
  }
  return(c(varname, missing, pval, na.indicator))
}
```

#### interaction selector
```{r}
interaction_deviance_selector = function (mytrain, selected_predictors, cat_predictors, num_predictors, alpha = 0.05) 
{
  model0 = glm(formula = paste("suppdem ~ ", do.call(paste, c(as.list(selected_predictors), sep = " + "))),
            family = binomial, data = mytrain)
  pval = alpha
  varname = ""
  all_interactions = c()
  for (cat_var in cat_predictors){
    for (num_var in num_predictors){
      int_var = paste(cat_var, num_var, sep=":")
      if (int_var %in% selected_predictors) next
      preds = c(selected_predictors, int_var)
      model = glm(formula = paste("suppdem ~ ", do.call(paste, c(as.list(preds), sep = " + "))),
            family = binomial, data = mytrain)
      if (anova(model0, model, test="Chisq")$`Pr(>Chi)`[2] < pval){
        varname = int_var
        pval = anova(model0, model, test="Chisq")$`Pr(>Chi)`[2]
      }
    }
  }
  return(c(varname, pval))
}
```

## Abstract
In this paper, we explore various logistic modeling methods to analyze a self-reported dataset from _BlueLabs.com_ to create a prediction model to test wheter a voter would support fot the Democratic party candidate in 2016. Our final selected prediction model is the unsmoothed binomial logistic model with selected predictors and interaction terms. Single predictors are sequentially chosen via a stepwise-deviance selection method; once there are no more single predictors to add, we also incorporate interaction terms of categorical and numerical terms using Chi-squared likelihood ratio tests. We compare our prediction results to those of a basic model without the interaction terms, the same full model trained on smoothed predictors, similar binomial models with probit and complementary-log-log link functions, and cross-validated random forest model. However, lest we overfit on our train data, we find that the aforementioned binomial logistic best predicts whether a voter supported the Democratic candidate in 2016.

## Introdcution
Before we delve into analyzing the dataset, we download various R packages, import the provided datasets, understand dependent and independent variables, and execute appropriate data transformations and imputations. Our train dataset cosists of fourty-eight predictors: our dependent variable and binary Y/N indicator of "suppdem" and 47 other numerical and categorical predictor variables. A summary statistics on our raw train dataset reveals many important information: (1) there does not seem to be glaring data entry mistakes (i.e. categorical, binary variables having non-binary inputs or certain variables having values outside the logical scope of range such as age of 200), (2) indepdent predictor variables "age", "education", "cnty_pct_religious", and "cnty_pct_evangelical" have missing values, and (3) the three numerical variables "density_rural", "density_suburban", and "density_urban" totals to 1 for each data entry. 

To address the aforementioned issues, we first transform our "suppdem" variable into 1's and 0's and then mean-impute our missing numerical values for "age", "cnty_pct_religious", and "cnty_pct_evangelical" with missing value inidcators, using the *na_convert_mean* function provided in lecture notes. For the "education" predictor variable, however, we decided to change missing values by creating a new level "none" within the categorical predictor that will replace "NA". By doing so, we eliminate the need to also include missing value indicator as we did for numerical predictor variables. In order to address the issue of perect multicolinearity between "density_rural", "density_suburban", and "density_urban", we have decided to remove "density_urban" in order to give one more free degree of freedom for our models to train on. Also, this is not arbitrary, rather we later find that "density_rural" and "density_suburban" will be selected prior to "density_urban" in our model.

## Model Selection
Since we had been using step-wise deviance selection method throughout our semester, we decided to implement *stepwise_deviance_selector* function. The inputs of this fucntion is: (1) the training dataset, (2) an array of pre-selected predictors on top of which a new predictor variable is added to, (3) a default boolean of "log" that will allow you to log-transform income measures of "ppi" and "census_median_income", and (4) a default alpha value of 0.05 that can be changed for different significance levels. The returned element from this *stepwise_deviance_selector* function is: (1) the name of the selected variable, if any, or a blank string, if no new variable can be added to the model, (2) a boolean indicator that indicates that the variable is missing certain data entrys, (3) the p-value of the likelihood ratio test of the deviances (which will return 0.05 or assigned alpha value, if no new predictor variable is added), and (4) another boolean inidcator that suggest whether or not to incorporate the missing value indicators should the newly selecte predictor have missing values.

Utilizing a do-while loop in R, the step-wise deviance selection method on a binomial logistic model returned the following predictors in sequential order: "combined_ethnicity_4way", "density_rural", "sex", "liberal_donor", "interest_in_religion", "cnty_pct_evangelical", "conservative_donor", "guns_1", "single", "education", "density_suburban", "contbrel_1", "num_children", "apparel_1", "cat_1", "cnty_pct_religious". Interestingly, the missing values indicators for both "cnty_pct_evangelical" and "cnty_pct_religious" are not included, as the inclusion of those indicators did not improve our model. You can see the full deviance table as shown below. This is our *basic_nointeraction_model* whose log-Loss score of 0.594 is impressive and predicted values on the test dataset beat the benchmark logistic model. 

Nonetheless, the basic model did not incorpoate any income measures, so we re-ran the same deviance-based selection method on the same training set with log-transformed values of "ppi" and "census_median_income".






## Introduction and Setup
### Download Packages & Load Dataset
```{r}
library(kyotil)
library(brglm)
library(aod)
library(nnet)
library(arm)
library(faraway)
library(Metrics)
library(randomForest)
library(rpart)
library(rpart.plot)
library(mgcv)
```

```{r}
train <- read.csv("./Data/train.csv")
test <- read.csv("./Data/test.csv")
```

## Explantory Data Analysis (EDA)
### Predictior Variables
Analysis on each predictors to come later
```{r}
names(train)
```

### Summary Statistics
get rid of one of 'density_rural', 'density_suburban', or 'density_urban' -> sums to 1
Change education to ordinal layers and not categorical values? OR change base-line to "no hs degree"
```{r}
summary(train)
```
### Conversion of Indepdent Variable
```{r}
mytrain <- train
mytrain$suppdem = as.numeric(mytrain$suppdem) - 1
```
### Conversion of Education
```{r}
levels(mytrain$education) <- c(levels(mytrain$education), "none")
mytrain$education[is.na(mytrain$education)] <- "none"
levels(test$education) <- c(levels(test$education), "none")
test$education[is.na(test$education)] <- "none"
```

## Model Selection
```{r}
mytrain.na = na.convert.mean(mytrain)
```
### Step-wise deviance on basic logistic model (binomial fam)
```{r}
mytrain.na = na.convert.mean(mytrain)
looptrain = within(mytrain.na, rm("density_urban"))
predictors = c(1)
while (stepwise_deviance_selector(looptrain, predictors)[1] != "")
{
  step = stepwise_deviance_selector(looptrain, predictors)
  predictors = c(predictors, step[1])
  # if selected column has NA
  if (step[2]){
    if (step[4]){
        predictors = c(predictors, paste(step[1], ".na", sep=""))
    }
  }
}
print(predictors)
predictors_nointeractions = predictors
```
### Comparison of Stepwise Models

```{r}
model00 = glm(formula = suppdem ~ 1, family = binomial, data = within(mytrain.na, rm("density_urban")))
model01 = glm(formula = suppdem ~ combined_ethnicity_4way, family = binomial, data = within(mytrain.na, rm("density_urban")))
model02 = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural, family = binomial, data = within(mytrain.na, rm("density_urban")))
model03 = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural + sex, family = binomial, data = within(mytrain.na, rm("density_urban")))
model04 = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural + sex + liberal_donor, family = binomial, data = within(mytrain.na, rm("density_urban")))
model05 = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural + sex + liberal_donor + interest_in_religion, family = binomial, data = within(mytrain.na, rm("density_urban")))
model06 = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural + sex + liberal_donor + interest_in_religion + cnty_pct_evangelical, family = binomial, data = within(mytrain.na, rm("density_urban")))
model07 = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural + sex + liberal_donor + interest_in_religion + cnty_pct_evangelical + conservative_donor, family = binomial, data = within(mytrain.na, rm("density_urban")))
model08 = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural + sex + liberal_donor + interest_in_religion + cnty_pct_evangelical + conservative_donor + guns_1, family = binomial, data = within(mytrain.na, rm("density_urban")))
model09 = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural + sex + liberal_donor + interest_in_religion + cnty_pct_evangelical + conservative_donor + guns_1 + single, family = binomial, data = within(mytrain.na, rm("density_urban")))
model10 = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural + sex + liberal_donor + interest_in_religion + cnty_pct_evangelical + conservative_donor + guns_1 + single + education, family = binomial, data = within(mytrain.na, rm("density_urban")))
model11 = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural + sex + liberal_donor + interest_in_religion + cnty_pct_evangelical + conservative_donor + guns_1 + single + education + density_suburban, family = binomial, data = within(mytrain.na, rm("density_urban")))
model12 = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural + sex + liberal_donor + interest_in_religion + cnty_pct_evangelical + conservative_donor + guns_1 + single + education + density_suburban + contbrel_1, family = binomial, data = within(mytrain.na, rm("density_urban")))
model13 = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural  +sex + liberal_donor + interest_in_religion + cnty_pct_evangelical + conservative_donor + guns_1 + single + education + density_suburban + contbrel_1 + num_children, family = binomial, data = within(mytrain.na, rm("density_urban")))
model14 = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural + sex + liberal_donor + interest_in_religion + cnty_pct_evangelical + conservative_donor + guns_1 + single + education + density_suburban + contbrel_1 + num_children + apparel_1, family = binomial, data = within(mytrain.na, rm("density_urban")))
model15 = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural + sex + liberal_donor + interest_in_religion + cnty_pct_evangelical + conservative_donor + guns_1 + single + education + density_suburban + contbrel_1 + num_children + apparel_1 + cat_1, family = binomial, data = within(mytrain.na, rm("density_urban")))
model16 = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural + sex + liberal_donor + interest_in_religion + cnty_pct_evangelical + conservative_donor + guns_1 + single + education + density_suburban + contbrel_1 + num_children + apparel_1 + cat_1 + cnty_pct_religious, family = binomial, data = within(mytrain.na, rm("density_urban")))
anova(model00, model01, model02, model03, model04, model05, model06, model07, model08, model09, model10, model11, model12, model13, model14, model15, model16, test="Chisq")
```

### Selected Model Prediction
```{r}
model_nointeraction = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural + sex + interest_in_religion + liberal_donor + guns_1 + density_suburban + conservative_donor + cnty_pct_evangelical + contbrel_1 + single + apparel_1 + education + cat_1 + num_children, family = binomial, data = within(mytrain.na, rm("density_urban")))

preds = predict(model_nointeraction, newdata = mytrain.na, type="response")
logLoss(mytrain.na$suppdem, preds)

test.na = na.convert.mean(test)
testpreds = predict(model15, newdata=test.na, type="response")
testpreds = setNames(data.frame(testpreds), c("suppdem"))
write.csv(testpreds, "./model_nointeraction_test.csv")
```

### Repeat above, but log median income
```{r}
mytrain.loginc.na <- mytrain.na
mytrain.loginc.na$census_median_income = log(mytrain.loginc.na$census_median_income)
mytrain.loginc.na$ppi = log(mytrain.loginc.na$ppi)

looptrain = within(mytrain.loginc.na, rm("density_urban"))
predictors = c(1)
while (stepwise_deviance_selector(looptrain, predictors, log = TRUE)[1] != "")
{
  step = stepwise_deviance_selector(looptrain, predictors, log = TRUE)
  predictors = c(predictors, step[1])
  # if selected column has NA
  if (step[2]){
    if (step[4]){
        predictors = c(predictors, paste(step[1], ".na", sep=""))
    }
  }
}
print(predictors)
predictors_loginc = predictors
```

### Testing out some interactions
```{r}
model32 = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural + sex + interest_in_religion + liberal_donor + guns_1 + density_suburban + conservative_donor + cnty_pct_evangelical + contbrel_1 + single + apparel_1 + education + cat_1 + num_children + education:ppi, family = binomial, data = within(mytrain.loginc.na, rm("density_urban")))
anova(model15, model32, test="Chisq")
```
### While-loop for Interactions
```{r}
looptrain = within(mytrain.loginc.na, rm("density_urban"))
categorical = c("combined_ethnicity_4way", "density_rural", "sex", "interest_in_religion", "liberal_donor", "guns_1", "density_suburban", "conservative_donor", "contbrel_1", "single", "apparel_1", "education", "cat_1")
numerical = c("num_children", "age", "census_median_income", "ppi", "cnty_pct_evangelical", "cnty_pct_religious") 
# predictors = predictors_nointeractions
while (interaction_deviance_selector(looptrain, predictors, categorical, numerical)[1] != "")
{
  step = interaction_deviance_selector(looptrain, predictors, categorical, numerical)
  predictors = c(predictors, step[1])
}
print(predictors)
predictors_interactions = predictors
```

```{r}
model_interaction = glm(formula = paste("suppdem ~ ", do.call(paste, c(as.list(predictors_interactions), sep = " + "))),
            family = binomial, data = looptrain)

# fitted vs standardized residuals
plot(fitted(model_interaction), rstandard(model_interaction),
           xlab="Averaged fitted probabilities",
           ylab="Deviance residuals", ylim = c(-3,3), pch=1,
           main="Fitted vs Standardized Residual Plot")
abline(h=2,lty=2,col="red")
abline(h=-2,lty=2,col="red")
# cooks
cooks = cooks.distance(model_interaction)
plot(cooks, type="h", lwd=2, xlab="Observation index", ylab="Cook's distances",
     main="Cook's Distances")
abline(h=1,lty=2,col="red")
# jackknifed
plot(fitted(model_interaction), rstudent(model_interaction),
           xlab="Averaged fitted probabilities",
           ylab="Jacknified residuals", ylim = c(-3,3), pch=1,
           main="Fitted vs Studentized Residual Plot")
abline(h=2,lty=2,col="red")
abline(h=-2,lty=2,col="red")
# Averaged
binnedplot(fitted(model_interaction), residuals(model_interaction,type="response"),xlab="Averaged fitted probabilities",ylab="Averaged residuals",pch=19, col.pts="red", cex.pts=1.5,main="Fitted vs residual plot")
abline(h=0,lty=2,col="green")


# Hosmer-Lemeshow test
# Partition fitted values into 10 groups
print(hosmerlem(model_interaction$y, fitted(model_interaction), g=10))
# Partition fitted values into 6 groups
print(hosmerlem(model_interaction$y, fitted(model_interaction), g=6))
```

### Selected Model w/ Interaction Prediction
```{r}
interaction.preds = predict(model_interaction, newdata=looptrain, type="response")

test.logna = na.convert.mean(test)
test.logna$census_median_income = log(test.logna$census_median_income)
test.logna$ppi = log(test.logna$ppi)
logLoss(looptrain$suppdem, interaction.preds)

testpreds = predict(model_interaction, newdata=test.logna, type="response")
testpreds = setNames(data.frame(testpreds), c("suppdem"))
write.csv(testpreds, "./model_interaction_test.csv")
```
### Parameter Smoothing
```{r}
model_smoothed = gam(suppdem~combined_ethnicity_4way+density_rural+liberal_donor+sex+s(cnty_pct_evangelical)
                     +interest_in_religion+conservative_donor+guns_1+conservative_donor+single+education+density_suburban+
                     contbrel_1+num_children+apparel_1+cat_1+s(cnty_pct_religious)+liberal_donor:census_median_income+
                       sex:age+combined_ethnicity_4way:census_median_income+density_rural:age+education:ppi+cat_1:age+
                       interest_in_religion:age+combined_ethnicity_4way:cnty_pct_evangelical+density_suburban:cnty_pct_religious, family = binomial, data = looptrain)
summary(model_smoothed)
```
### Testing smoothed model
```{r}
smooth.preds = predict(model_smoothed, newdata = looptrain, type="response")
logLoss(looptrain$suppdem, smooth.preds)

testpreds = predict(model_smoothed, newdata=test.logna, type="response")
testpreds = setNames(data.frame(testpreds), c("suppdem"))
write.csv(testpreds, "./model_interaction_test_smoothing.csv")
```
### GAM 
```{r}
GAM <- gam(suppdem ~ combined_ethnicity_4way+density_rural+liberal_donor+sex+s(cnty_pct_evangelical)
                     +interest_in_religion+conservative_donor+guns_1+conservative_donor+single+education+density_suburban+
                     contbrel_1+num_children+apparel_1+cat_1+s(cnty_pct_religious)+liberal_donor:census_median_income+
                       sex:age+combined_ethnicity_4way:census_median_income+density_rural:age+education:ppi+cat_1:age+
                       interest_in_religion:age+combined_ethnicity_4way:cnty_pct_evangelical+density_suburban:cnty_pct_religious
           , data = looptrain, family = binomial)
anova(model_interaction, GAM, test="Chisq")

```
### GAM 
```{r}
GAM <- gam(suppdem ~ combined_ethnicity_4way+density_rural+liberal_donor+sex+s(cnty_pct_evangelical)
                     +interest_in_religion+conservative_donor+guns_1+conservative_donor+single+education+density_suburban+
                     contbrel_1+num_children+apparel_1+cat_1+s(cnty_pct_religious)+liberal_donor:census_median_income+
                       sex:age+combined_ethnicity_4way:census_median_income+density_rural:age+education:ppi+cat_1:age+
                       interest_in_religion:age+combined_ethnicity_4way:cnty_pct_evangelical+density_suburban:cnty_pct_religious
           , data = looptrain, family = binomial)
anova(model_interaction, GAM, test="Chisq")

```
### Random Forest Cross Validation
```{r}
cvr = rfcv(na.roughfix((within(looptrain, rm("suppdem")))),looptrain$suppdem,step=0.9)
cbind(nvars=cvr$n.var, error.rate=cvr$error.cv)
```
### Random Forest
```{r}
sc.rf = randomForest(suppdem~combined_ethnicity_4way+density_rural+liberal_donor+sex+cnty_pct_evangelical
                     +interest_in_religion+conservative_donor+guns_1+conservative_donor+single+education+density_suburban+
                     contbrel_1+num_children+apparel_1+cat_1+cnty_pct_religious, m.try=32, data=looptrain, na.action=na.roughfix)
sc.rf
```

```{r}
rf.preds = predict(sc.rf, newdata=looptrain)

test.logna = na.convert.mean(test)
test.logna$census_median_income = log(test.logna$census_median_income)
test.logna$ppi = log(test.logna$ppi)
logLoss(looptrain$suppdem, rf.preds)

testpreds = predict(sc.rf, newdata=test.logna)
testpreds = setNames(data.frame(testpreds), c("suppdem"))
write.csv(testpreds, "./model_rf.csv")
```



### Model Comparisons 
```{r}
model_basic = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural + sex + interest_in_religion + liberal_donor + guns_1 
                  + density_suburban + conservative_donor + cnty_pct_evangelical + contbrel_1 + single + apparel_1 + education 
                  + cat_1 + num_children, family = binomial, data = within(mytrain.loginc.na, rm("density_urban")))
model_basic_probit = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural + sex + interest_in_religion + liberal_donor + guns_1 
                         + density_suburban + conservative_donor + cnty_pct_evangelical + contbrel_1 + single + apparel_1 + education 
                         + cat_1 + num_children, family = binomial(link="probit"), data = within(mytrain.loginc.na, rm("density_urban")))
model_basic_cloglog = glm(formula = suppdem ~ combined_ethnicity_4way + density_rural + sex + interest_in_religion + liberal_donor + guns_1
                          + density_suburban + conservative_donor + cnty_pct_evangelical + contbrel_1 + single + apparel_1 + education
                          + cat_1 + num_children, family = binomial(link="cloglog"), data = within(mytrain.loginc.na, rm("density_urban")))
model_interaction = glm(formula = paste("suppdem ~ ", do.call(paste, c(as.list(predictors_interactions), sep = " + ")))
                        , family = binomial, data = within(mytrain.loginc.na, rm("density_urban")))
model_interaction_probit = glm(formula = paste("suppdem ~ ", do.call(paste, c(as.list(predictors_interactions), sep = " + ")))
                               , family = binomial(link="probit"), data = within(mytrain.loginc.na, rm("density_urban")))
model_interaction_cloglog = glm(formula = paste("suppdem ~ ", do.call(paste, c(as.list(predictors_interactions), sep = " + ")))
                                , family = binomial(link="cloglog"), data = within(mytrain.loginc.na, rm("density_urban")))
anova(model_basic, model_basic_probit, model_basic_cloglog, model_interaction, model_interaction_probit, model_interaction_cloglog, test="Chisq")
```

